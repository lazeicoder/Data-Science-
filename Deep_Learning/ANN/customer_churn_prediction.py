# -*- coding: utf-8 -*-
"""notebook3e8f301ca3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/anuragdev212/notebook3e8f301ca3.285aaba9-e793-40b7-b9e8-0a50dbecfe7c.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251205/auto/storage/goog4_request%26X-Goog-Date%3D20251205T191324Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D56572fbc7402cb6b774157ab406baafded76dad7251122b3f1c37063b037629b841fbdc8f80d3ecd8b0043ef4c482656ad52ad8d3afc6568db3f64639d4601be2cad1b0e63684b90cdc776a9939e7c7e57a93e95a7ccefe7efcde9688e319f961c7075673d8572de640615d86a6c4737ce04bf83997ef899ef043035d999eed30f7119c48d0bba6010d9fa954fb6d4abbb2d091c1db44c386a99bfd82e5011db933b9783a980902f473ecb63a872f5ba9903b641364dae66895812f22b901598623a47b714e810c4e6f0c5cb3625d7139f20f366bd257ddcdbd711654a9ba047ef22cb1e6a7f5b4c447f7aac72723d002850c1627cb7337608aead7e4139f0a0
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
rjmanoj_credit_card_customer_churn_prediction_path = kagglehub.dataset_download('rjmanoj/credit-card-customer-churn-prediction')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

data_frame = pd.read_csv('/kaggle/input/credit-card-customer-churn-prediction/Churn_Modelling.csv')

print(data_frame.head())
print(data_frame.shape)

data_frame.info()

data_frame.duplicated().sum()

data_frame['Exited'].value_counts()

data_frame['Geography'].value_counts()

data_frame['Gender'].value_counts()

data_frame.drop(columns=['RowNumber', 'CustomerId', 'Surname'], inplace=True)

print(data_frame.head())

data_frame

data_frame = pd.get_dummies(data_frame, columns=['Geography', 'Gender'], drop_first=True)

print(data_frame.head())

X = data_frame.drop(columns=['Exited'])
Y = data_frame['Exited']
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)

print(X.head())

print(Y.head())

print(X_train.shape)

print(Y_train.shape)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(X_train_scaled)

import tensorflow
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

model1 = Sequential()

# Tip-1: Keeping the activation function as ReLu gives better, accurate results
# Tip-2: Increase the number of nodes in the hidden layer to increase accuracy
# Tip-3: Increase the number of hidden layers to increase accuracy

# Note: Increasing # of hidden layers more than requirement results in overfitting

# Initializing the hidden layer of the Artificial Neural Network
# model1.add(Dense(3, activation="sigmoid", input_dim=11))

# Layer-1
model1.add(Dense(11, activation="relu", input_dim=11))

# Layer-2
model1.add(Dense(6, activation="relu"))

# Initializing the output layer
model1.add(Dense(1, activation="sigmoid"))

model1.summary()

# The model is compiled
# Setting up the model with Binary Cross Entropy loss function
# Using Adam Optimizer
# Metrics show the metrics mentioned besides Loss

model1.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])

history = model1.fit(X_train_scaled, Y_train, epochs=100, validation_split=0.2)
# Increase the number of epochs to increase model prediction accuracy

# This gives the weights of the first layer
model1.layers[0].get_weights()[0]

# This gives the biases of the layer
model1.layers[0].get_weights()[1]

# This gives the weights of the second layer
model1.layers[1].get_weights()[0]

# This gives the biases of the second layer
model1.layers[1].get_weights()[1]

# Prediction
y_log = model1.predict(X_test_scaled)

y_pred = np.where(y_log > 0.5, 1, 0)

from sklearn.metrics import accuracy_score
accuracy_score(Y_test, y_pred)

import matplotlib.pyplot as plt

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])