# -*- coding: utf-8 -*-
"""Backpropagation_custom.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-JMyOgMgcxmpSM58IK_t9RN7Dk8wIWpa
"""

import numpy as np
import pandas as pd

data_frame = pd.DataFrame([[8,8,4],[7,9,5],[6,10,6],[5,12,7]], columns=['cgpa','profile_score','lpa'])

print(data_frame)

# layer_dims -> architecture of the neural network
# This function will create 9 weights and biases
# Also, initializes with w->1 and b->0

def initialize_parameters(layer_dims):

  np.random.seed(3)
  parameters = {}
  L = len(layer_dims)

  for i in range(1, L):

    parameters['W' + str(i)] = np.ones((layer_dims[i-1], layer_dims[i]))*0.1
    parameters['b' + str(i)] = np.zeros((layer_dims[i], 1))

  return parameters

initialize_parameters([2, 2, 1])

# Calculates the given neurons output

def linear_forward(A_prev, W, b):

  Z = np.dot(W.T, A_prev) + b
  return Z

# Forward Propagation

def L_layer_forward(X, parameters):

  A = X
  L = len(parameters) // 2  # number of layers in neural network

  for l in range(1, L+1):
    A_prev = A
    w1 = parameters['W' + str(l)]
    b1 = parameters['b' + str(l)]

    # print(f"A{str(l-1)}: {A_prev}")
    # print(f"W{str(l)}: {w1}")
    # print(f"b{str(l)}: {b1}")
    # print("--"*20)

    A = linear_forward(A_prev, w1, b1)
    # print(f"A{str(l)}: {A}")
    # print("**"*20)

  # returns final node and prev node values
  return A, A_prev

X = data_frame[['cgpa', 'profile_score']].values[0].reshape(2, 1) # Shape(no. of features, no. of training examples)
y = data_frame[['lpa']].values[0][0]

# Parameter initialization
parameters = initialize_parameters([2, 2, 1])

# print(X)
# print(y)

# print(parameters)

y_hat, A1 = L_layer_forward(X, parameters)

print(f"Predicted value: {y_hat[0][0]}")
y_hat = y_hat[0][0]


print(f"Prev. neuron output: {A1}")

print((y - 0.32)**2)

def update_parameters(parameters,y,y_hat,A1,X):
  parameters['W2'][0][0] = parameters['W2'][0][0] + (0.001 * 2 * (y - y_hat)*A1[0][0])
  parameters['W2'][1][0] = parameters['W2'][1][0] + (0.001 * 2 * (y - y_hat)*A1[1][0])
  parameters['b2'][0][0] = parameters['W2'][1][0] + (0.001 * 2 * (y - y_hat))

  parameters['W1'][0][0] = parameters['W1'][0][0] + (0.001 * 2 * (y - y_hat)*parameters['W2'][0][0]*X[0][0])
  parameters['W1'][0][1] = parameters['W1'][0][1] + (0.001 * 2 * (y - y_hat)*parameters['W2'][0][0]*X[1][0])
  parameters['b1'][0][0] = parameters['b1'][0][0] + (0.001 * 2 * (y - y_hat)*parameters['W2'][0][0])

  parameters['W1'][1][0] = parameters['W1'][1][0] + (0.001 * 2 * (y - y_hat)*parameters['W2'][1][0]*X[0][0])
  parameters['W1'][1][1] = parameters['W1'][1][1] + (0.001 * 2 * (y - y_hat)*parameters['W2'][1][0]*X[1][0])
  parameters['b1'][1][0] = parameters['b1'][1][0] + (0.001 * 2 * (y - y_hat)*parameters['W2'][1][0])

update_parameters(parameters,y,y_hat,A1,X)

print(parameters)

X = data_frame[['cgpa', 'profile_score']].values[1].reshape(2, 1) # Shape(no. of features, no. of training examples)
y = data_frame[['lpa']].values[0][0]

y_hat, A1 = L_layer_forward(X, parameters)

y_hat = y_hat[0][0]

update_parameters(parameters,y,y_hat,A1,X)

print(parameters)

X = data_frame[['cgpa', 'profile_score']].values[2].reshape(2, 1) # Shape(no. of features, no. of training examples)
y = data_frame[['lpa']].values[0][0]

y_hat, A1 = L_layer_forward(X, parameters)

y_hat = y_hat[0][0]

update_parameters(parameters,y,y_hat,A1,X)

print(parameters)

X = data_frame[['cgpa', 'profile_score']].values[3].reshape(2, 1) # Shape(no. of features, no. of training examples)
y = data_frame[['lpa']].values[0][0]

y_hat, A1 = L_layer_forward(X, parameters)

y_hat = y_hat[0][0]

update_parameters(parameters,y,y_hat,A1,X)

print(parameters)

# Epoch Implementation

parameters = initialize_parameters([2,2,1])
epochs = 5

for i in range(epochs):

  Loss = []

  for j in range(data_frame.shape[0]):

    X = data_frame[['cgpa','profile_score']].values[j].reshape(2,1)
    y = data_frame[['lpa']].values[j][0]

    # Parameter Initialization

    y_hat,A1 = L_layer_forward(X,parameters)
    y_hat = y_hat[0][0]

    update_parameters(parameters,y,y_hat,A1,X)

    Loss.append((y-y_hat)**2)

  print(f"Epoch - {i+1} Loss - {np.array(Loss).mean()}")

print(parameters)