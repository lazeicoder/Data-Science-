# -*- coding: utf-8 -*-
"""gradient_descent_basic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1epJcDEMla67v2vZQ2onAbn5FTmHcxwPj
"""

from sklearn.datasets import make_regression
import numpy as np

X, Y = make_regression(n_samples=4, n_features=1, n_informative=1, n_targets=1, noise=80, random_state=13)

import matplotlib.pyplot as plt
plt.scatter(X, Y)

from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X, Y)

model.coef_

model.intercept_

plt.scatter(X, Y)
plt.plot(X, model.predict(X), color='red')

# Let's apply Gradient Descent assuming slope is constant m = 78.35063668
# and let's assume the starting value for intercept b = 0

y_pred = ((78.35063668 * X) + 0).reshape(4)

plt.scatter(X, Y)

# The regression model give the best-fit line using ordinary least square
plt.plot(X, model.predict(X), color='red', label='OLS')

# This gives the line given using the initial stage of the Gradient Descent model
plt.plot(X, y_pred, color='blue', label='b = 0')
plt.legend()
plt.show()

m = 78.35063668
b = 0

# First Iteration

print(X.ravel())
print(Y)

loss_slope = -2 * np.sum(Y - m * X.ravel() - b)
print(loss_slope)

learning_rate = 0.1

step_size = loss_slope * learning_rate
print(step_size)

b = b - step_size
print(b)

y_pred1 = ((78.35063668 * X) + b).reshape(4)

plt.scatter(X, Y)

plt.plot(X, model.predict(X), color = 'red', label='OLS')
plt.plot(X, y_pred, color='blue', label='b = 0')
plt.plot(X, y_pred1, color='green', label='b = {}'.format(b))

plt.legend()
plt.show()

# Second Iteration

loss_slope = -2 * np.sum(Y - m * X.ravel() - b)
print(loss_slope)

step_size = loss_slope * learning_rate
print(step_size)

b = b - step_size
print(b)

y_pred2 = ((78.35063668 * X) + b).reshape(4)

plt.scatter(X, Y)

plt.plot(X, model.predict(X), color = 'red', label='OLS')
plt.plot(X, y_pred, color='blue', label='b = 0')
plt.plot(X, y_pred2, color='violet', label='b = {}'.format(b))

plt.legend()
plt.show()

# Third Iteration

loss_slope = -2 * np.sum(Y - m * X.ravel() - b)
print(loss_slope)

step_size = loss_slope * learning_rate
print(step_size)

b = b - step_size
print(b)

y_pred3 = ((78.35063668 * X) + b).reshape(4)

plt.scatter(X, Y)

plt.plot(X, model.predict(X), color = 'red', label='OLS')
plt.plot(X, y_pred, color='blue', label='b = 0')
plt.plot(X, y_pred3, color='violet', label='b = {}'.format(b))

plt.legend()
plt.show()

epochs = 100
b = -100

lr = 0.01


for i in range(epochs) :
  loss_slope = -2 * np.sum(Y - m * X.ravel() - b)
  b = b - (lr * loss_slope)

  y_pred = (m * X) + b

  plt.plot(X, y_pred)

plt.scatter(X, Y)