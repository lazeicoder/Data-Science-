# -*- coding: utf-8 -*-
"""Custom_Gradient_Descent_Class.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z-U0Th0AReBkso1y9MvCfS-l-pmVHKSP
"""

from sklearn.datasets import make_regression
import matplotlib.pyplot as plt
import numpy as np

X, Y = make_regression(n_samples=100, n_features=1, n_informative=1, n_targets=1, noise=20, random_state=13)

plt.scatter(X, Y)

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=13)

from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train,Y_train)

m = model.coef_[0]
print(f"Slope is: {m}")

b = model.intercept_
print(f"Intercept is: {b}")

plt.scatter(X_test,Y_test)
plt.plot(X_test, model.predict(X_test), color='red')

from sklearn.model_selection import cross_val_score
np.mean(cross_val_score(model,X_test,model.predict(_test),scoring='r2',cv=10))

y_pred = model.predict(X_test)
from sklearn.metrics import r2_score
r2_score(Y_test,y_pred)

class GDRegressor:

  def __init__(self, learning_rate, epochs):
    self.m = 100
    self.b = -120
    self.lr = learning_rate
    self.epochs = epochs

  def fit(self, X, Y):
    # Calculate m and b using GD
    for i in range(self.epochs):
      loss_slope_b = -2 * np.sum(Y - self.m*X.ravel() - self.b)
      loss_slope_m = -2 * np.sum((Y - self.m*X.ravel() - self.b) * X.ravel())

      self.b = self.b - (self.lr * loss_slope_b)
      self.m = self.m - (self.lr * loss_slope_m)

    print(f"Slope is: {self.m}")
    print(f"Intercept is: {self.b}")

  def predict(self, X):
    return self.m * X + self.b

grad_descent = GDRegressor(0.001, 50)

grad_descent.fit(X_train, Y_train)

grad_descent.predict(X_test)

Y_pred = grad_descent.predict(X_test)
from sklearn.metrics import r2_score
r2_score(Y_test, Y_pred)